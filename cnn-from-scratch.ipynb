{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <span style=\"color:crimson;\"><p style=\"text-align:center;\">CONVOLUTIONAL NEURAL NETWORK FROM SCRATCH - FOR BEGINERS","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"<u><span style=\"font-size:28px;\">WHAT IS CNN:</span></u>\n\n* <p style=\"font-size:20px;\">CNNs, like neural networks, are made up of neurons with learnable weights and biases. Each neuron receives several inputs, takes a weighted sum over them, pass it through an activation function and responds with an output. The whole network has a loss function and all the tips and tricks that we developed for neural networks still apply on CNNs.</p>\n\n* <p style=\"font-size:20px;\">It processes data that has a grid-like arrangement then extracts important features. One huge advantage of using CNNs is that you don't need to do a lot of pre-processing on images.</p>\n\n<u><span style=\"font-size:28px;\">HOW CNN WORKS:</span></u>\n\n![](https://miro.medium.com/max/1750/0*0lrC8RWrShYm8U9L.png)\n\n* <p style=\"font-size:20px;\">When you're working with data in a CNN, each layer returns activation maps. These maps point out important features in the data set. If you gave the CNN an image, it'll point out features based on pixel values, like colors, and give you an activation function.</p>\n\n* <p style=\"font-size:20px;\">Usually with images, a CNN will initially find the edges of the picture. Then this slight definition of the image will get passed to the next layer. Then that layer will start detecting things like corners and color groups. Then that image definition will get passed to the next layer and the cycle continues until a prediction is made.</p>\n\n* <p style=\"font-size:20px;\">The last layer of a CNN is the classification layer which determines the predicted value based on the activation map. If you pass a handwriting sample to a CNN, the classification layer will tell you what letter is in the image.</p>\n\n<u><span style=\"font-size:28px;\">Types of Layers in CNN</span></u>\n\n* <p style=\"font-size:20px;\">INPUT LAYER</p>\n* <p style=\"font-size:20px;\">CONVOLUTIONAL LAYER</p>\n* <p style=\"font-size:20px;\">ACTIVATION FUNCTION LAYER</p>\n* <p style=\"font-size:20px;\">POOLING LAYER</p>\n* <p style=\"font-size:20px;\">FULLY-CONNECTED LAYER</p>\n\n<u><span style=\"font-size:28px;\">POOLING LAYERS IN CNN:</span></u>\n\n* <p style=\"font-size:20px;\">The pooling operation involves sliding a two-dimensional filter over each channel of feature map and summarising the features lying within the region covered by the filter.</p>\n\n* <p style=\"font-size:20px;\">For a feature map having dimensions nh x nw x nc, the dimensions of output obtained after a pooling layer is</p>\n\n* <p style=\"font-size:20px;\">(nh - f + 1) / s x (nw - f + 1)/s x nc</p>\n\n                    nh - height of feature map\n                    nw - width of feature map\n                    nc - number of channels in the feature map\n                    f  - size of filter\n                    s  - stride length\n                    \n<u><span style=\"font-size:28px;\">Why to use Pooling Layers?</span></u>\n\n* <p style=\"font-size:20px;\">Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network.</p>\n\n* <p style=\"font-size:20px;\">The pooling layer summarises the features present in a region of the feature map generated by a convolution layer. So, further operations are performed on summarised features instead of precisely positioned features generated by the convolution layer. This makes the model more robust to variations in the position of the features in the input image.</p>\n\n<u><span style=\"font-size:28px;\">Types of Pooling Layers</span></u>\n\n<p style=\"font-size:20px;\"><b>MAX POOLING:</b> Max pooling is a pooling operation that selects the maximum element from the region of the feature map covered by the filter. Thus, the output after max-pooling layer would be a feature map containing the most prominent features of the previous feature map.</p>\n\n![](https://media.geeksforgeeks.org/wp-content/uploads/20190721025744/Screenshot-2019-07-21-at-2.57.13-AM.png)\n\n<p style=\"font-size:20px;\"><b>AVERAGE POOLING:</b> Average pooling computes the average of the elements present in the region of feature map covered by the filter. Thus, while max pooling gives the most prominent feature in a particular patch of the feature map, average pooling gives the average of features present in a patch.</p>\n\n![](https://media.geeksforgeeks.org/wp-content/uploads/20190721030705/Screenshot-2019-07-21-at-3.05.56-AM.png)","metadata":{}},{"cell_type":"markdown","source":"##### <center> (IMAGE & CONTENT SOURCE - GEEKSFORGEEKS)","metadata":{}},{"cell_type":"markdown","source":"-----------","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:crimson;\"><p style=\"text-align:center;\">CONVOLUTIONAL NEURAL NETWORK IMPLEMENTATION - PYTHON","metadata":{}},{"cell_type":"code","source":"# IMPORT REQUIRED LIBRARIES:\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"print(tf.__version__)","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"2.4.1\n","output_type":"stream"}]},{"cell_type":"code","source":"#IMPORT FASHION MNIST DATASET:\n\nfashion_mnist = keras.datasets.fashion_mnist","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#SPLIT THE DATA INTO TEST AND TRAIN:\n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n32768/29515 [=================================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26427392/26421880 [==============================] - 1s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n8192/5148 [===============================================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4423680/4422102 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"#NORMALIZE THE IMAGES:\n\ntrain_images = train_images/255.0\ntest_images = test_images/255.0","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#GET THE SHAPE OF THE IMAGES:\ntrain_images[0].shape","metadata":{"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(28, 28)"},"metadata":{}}]},{"cell_type":"code","source":"#RESHAPE THE IMAGES FROM 28, 28 TO (28, 28, 1)\n\ntrain_images = train_images.reshape(len(train_images), 28, 28, 1)\ntest_images = test_images.reshape(len(test_images), 28, 28, 1)","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def build_model(hp):  \n    model = keras.Sequential([\n    \n    # First CNN Layer:\n    keras.layers.Conv2D(\n        filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),\n        kernel_size=hp.Choice('conv_1_kernel', values = [3,5]),\n        activation='relu',\n        input_shape=(28,28,1)\n    ),\n        \n    # Second CNN Layer:\n    keras.layers.Conv2D(\n        filters=hp.Int('conv_2_filter', min_value=32, max_value=64, step=16),\n        kernel_size=hp.Choice('conv_2_kernel', values = [3,5]),\n        activation='relu'\n    ),\n    \n    # Flatten Layer:    \n    keras.layers.Flatten(),\n    \n    # Dense Layer:    \n    keras.layers.Dense(\n        units=hp.Int('dense_1_units', min_value=32, max_value=128, step=16),\n        activation='relu'\n    ),\n    \n    #Output Layer    \n    keras.layers.Dense(10, activation='softmax')\n    ])\n    \n    #Compile the Model\n    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n  \n    return model","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# We are importing randomsearch - this will tell us which parameters will be best:\nfrom kerastuner import RandomSearch\nfrom kerastuner.engine.hyperparameters import HyperParameters","metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#Run the Random Search:\ntuner_search = RandomSearch(build_model, objective = 'val_accuracy', max_trials=5, directory='output', project_name=\"fashion_MNIST\")","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#This by default runs for only \"2 EPOCHS\". we can also change the value if we want.\n\ntuner_search.search(train_images, train_labels, epochs=3, validation_split=0.1)","metadata":{"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Trial 5 Complete [00h 15m 05s]\nval_accuracy: 0.1054999977350235\n\nBest val_accuracy So Far: 0.9116666913032532\nTotal elapsed time: 00h 34m 30s\n","output_type":"stream"}]},{"cell_type":"code","source":"model = tuner_search.get_best_models(num_models=1)[0]","metadata":{"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 26, 26, 64)        640       \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 24, 24, 48)        27696     \n_________________________________________________________________\nflatten (Flatten)            (None, 27648)             0         \n_________________________________________________________________\ndense (Dense)                (None, 48)                1327152   \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                490       \n=================================================================\nTotal params: 1,355,978\nTrainable params: 1,355,978\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"#RETRAIN ALL THE IMAGES CONSIDERING THE ABOVE BEST MODEL:\n\nmodel.fit(train_images, train_labels, epochs=10, validation_split=0.1, initial_epoch=3)","metadata":{"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Epoch 4/10\n1688/1688 [==============================] - 112s 66ms/step - loss: 0.1564 - accuracy: 0.9425 - val_loss: 0.2528 - val_accuracy: 0.9120\nEpoch 5/10\n1688/1688 [==============================] - 110s 65ms/step - loss: 0.1184 - accuracy: 0.9564 - val_loss: 0.2881 - val_accuracy: 0.9122\nEpoch 6/10\n1688/1688 [==============================] - 110s 65ms/step - loss: 0.0871 - accuracy: 0.9690 - val_loss: 0.3214 - val_accuracy: 0.9077\nEpoch 7/10\n1688/1688 [==============================] - 113s 67ms/step - loss: 0.0623 - accuracy: 0.9773 - val_loss: 0.3545 - val_accuracy: 0.9133\nEpoch 8/10\n1688/1688 [==============================] - 110s 65ms/step - loss: 0.0464 - accuracy: 0.9830 - val_loss: 0.3628 - val_accuracy: 0.9122\nEpoch 9/10\n1688/1688 [==============================] - 110s 65ms/step - loss: 0.0384 - accuracy: 0.9862 - val_loss: 0.4048 - val_accuracy: 0.9162\nEpoch 10/10\n1688/1688 [==============================] - 110s 65ms/step - loss: 0.0274 - accuracy: 0.9905 - val_loss: 0.4326 - val_accuracy: 0.9058\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f793c0bd050>"},"metadata":{}}]},{"cell_type":"markdown","source":"# THIS IS HOW CNN MODEL IS CREATED AND OPTIMIZED USING KERAS","metadata":{}},{"cell_type":"markdown","source":"## IF YOU GUYS FINDS THIS INSIGHTFUL, PLEASE GIVE AN UPVOTE.","metadata":{}}]}